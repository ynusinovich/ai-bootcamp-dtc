{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 1 — Q4–Q6 (Podcasts ingestion + chunking + search)\n\nThis notebook fetches DataTalks.Club podcasts, chunks paragraphs, indexes with **minsearch**, and answers a query."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# If `python-frontmatter` isn't installed yet, uncomment to add it to your env:\n# !uv add python-frontmatter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, io, json, requests, traceback\nfrom typing import Iterable, Callable, Dict, Any, List\nfrom dataclasses import dataclass\n\nimport frontmatter\nfrom minsearch import Index\n\nGITHUB_API_DIR = \"https://api.github.com/repos/DataTalksClub/datatalksclub.github.io/contents/_podcast?ref=main\"\nRAW_REPO_URL = \"https://raw.githubusercontent.com/DataTalksClub/datatalksclub.github.io/main/_podcast\"\nSESSION = requests.Session()\nSESSION.headers.update({\"User-Agent\": \"ai-bootcamp-notebook\"})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def list_podcast_files() -> list[dict]:\n    r = SESSION.get(GITHUB_API_DIR, timeout=30)\n    r.raise_for_status()\n    items = r.json()\n    return [it for it in items if it.get(\"type\") == \"file\" and it.get(\"name\",\"\").lower().endswith((\".md\",\".mdx\"))]\n\n@dataclass\nclass RawFile:\n    filename: str\n    content: str\n\ndef fetch_podcast_markdowns(files: list[dict]) -> list[RawFile]:\n    data = []\n    for it in files:\n        url = it.get(\"download_url\") or f\"{RAW_REPO_URL}/{it['name']}\"\n        name = it[\"name\"]\n        try:\n            resp = SESSION.get(url, timeout=60)\n            resp.raise_for_status()\n            content = resp.text.strip()\n            data.append(RawFile(filename=name, content=content))\n        except Exception as e:\n            print(f\"Failed to fetch {name}: {e}\")\n            traceback.print_exc()\n    return data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Q4 — Download the podcast data and count records"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "files = list_podcast_files()\nraw_docs = fetch_podcast_markdowns(files)\nnum_records = len(raw_docs)\nprint(f\"Q4: number of podcast records: {num_records}\")\nfor rf in raw_docs[:3]:\n    print(\" -\", rf.filename)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_frontmatter(raw_docs: list[RawFile]) -> list[dict]:\n    parsed = []\n    for rf in raw_docs:\n        post = frontmatter.loads(rf.content)\n        d = post.to_dict()\n        d[\"filename\"] = rf.filename\n        if \"content\" not in d:\n            d[\"content\"] = post.content if isinstance(post.content, str) else str(post.content)\n        parsed.append(d)\n    return parsed\n\ndocs = parse_frontmatter(raw_docs)\nprint(\"Sample keys:\", sorted(set().union(*[set(d.keys()) for d in docs])))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Q5 — Chunk by paragraphs (size=30, overlap=15) and count chunks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Any\n\ndef split_paragraphs(text: str) -> list[str]:\n    parts = [p.strip() for p in text.replace(\"\\r\\n\",\"\\n\").split(\"\\n\\n\")]\n    return [p for p in parts if p]\n\ndef sliding_window(seq: list[Any], size: int, step: int) -> list[dict]:\n    if size <= 0 or step <= 0:\n        raise ValueError(\"size and step must be positive\")\n    n = len(seq)\n    results = []\n    i = 0\n    while i < n:\n        batch = seq[i:i+size]\n        if not batch:\n            break\n        results.append({\"start\": i, \"content\": batch})\n        if i + size >= n:\n            break\n        i += step\n    return results\n\ndef chunk_paragraphs(documents: list[dict], size: int = 30, overlap: int = 15, content_field: str = \"content\") -> list[dict]:\n    step = max(1, size - overlap)\n    out = []\n    for d in documents:\n        text = d.get(content_field, \"\") or \"\"\n        paragraphs = split_paragraphs(text)\n        windows = sliding_window(paragraphs, size=size, step=step)\n        for w in windows:\n            chunk = d.copy()\n            chunk[\"content\"] = \"\\n\\n\".join(w[\"content\"])\n            chunk[\"para_start\"] = w[\"start\"]\n            out.append(chunk)\n    return out\n\nchunks = chunk_paragraphs(docs, size=30, overlap=15, content_field=\"content\")\nprint(f\"Q5: number of chunks: {len(chunks)}\")\nprint(chunks[0][\"filename\"], \"paras-start:\", chunks[0][\"para_start\"])\nprint(chunks[0][\"content\"][:300].replace(\"\\n\",\" \") + \" ...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Q6 — Index with minsearch and query"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "index = Index(text_fields=[\"content\", \"title\", \"filename\", \"description\"])\nindex.fit(chunks)\n\nquery = \"how do I make money with AI?\"\nresults = index.search(query=query, num_results=10)\n\nprint(\"Top 3 results:\")\nfor i, r in enumerate(results[:3], 1):\n    print(f\"{i}. {r.get('title')!r} — {r.get('filename')}\")\n\nfirst_title = results[0].get(\"title\") if results else None\nfirst_filename = results[0].get(\"filename\") if results else None\nprint(\"\\nQ6: first episode in results:\", first_title or \"<no title>\", \"(\", first_filename, \")\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}