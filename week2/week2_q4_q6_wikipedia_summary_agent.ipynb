{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97e7eb4",
   "metadata": {},
   "source": [
    "# Week 2 - Part 2: Summary Agent (LangChain 1.0 + OpenAI) - Local Summary Search\n",
    "\n",
    "This notebook uses **LangChain 1.0's `create_agent` API** and **OpenAI** to implement an agent with three tools:\n",
    "1. `fetch_web_page(url)` - fetches and cleans page text.\n",
    "2. `save_summary(url, summary)` - validates (Pydantic) and saves a summary to `summaries/` as JSON.\n",
    "3. `search_summaries(query, k)` - searches previously saved summaries (no web calls) and returns the most relevant records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7a32e",
   "metadata": {},
   "source": [
    "## Question 4 - Framework and LLM Provider\n",
    "\n",
    "- Framework: **LangChain 1.0** (`create_agent`)\n",
    "- LLM Provider: **OpenAI** via `langchain-openai`\n",
    "\n",
    "We define tools, a system prompt with the requested rules, and enforce structured output (Pydantic) for final answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5614bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, pathlib, datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "\n",
    "from pydantic import BaseModel, HttpUrl, Field\n",
    "from pydantic import field_validator as pyd_field_validator \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create summaries directory\n",
    "SUM_DIR = pathlib.Path(\"summaries\")\n",
    "SUM_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Utility: safe filename from URL\n",
    "def slugify_url(url: str) -> str:\n",
    "    slug = re.sub(r'[^a-zA-Z0-9\\-_.]+', '_', url.strip())\n",
    "    return slug[:180]\n",
    "\n",
    "# Log tool calls\n",
    "TOOL_CALLS: List[str] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4f3f8",
   "metadata": {},
   "source": [
    "## Pydantic Models\n",
    "We validate saved summaries and the agent's final answer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryRecord(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str] = None\n",
    "    summary: str = Field(..., min_length=20, max_length=10000)\n",
    "    timestamp: str\n",
    "\n",
    "    @pyd_field_validator(\"summary\")\n",
    "    def check_word_count(cls, v: str):\n",
    "        words = v.split()\n",
    "        if len(words) > 240:\n",
    "            v = \" \".join(words[:240])\n",
    "            words = v.split()\n",
    "        if len(words) < 80:\n",
    "            raise ValueError(\"Summary must be at least 80 words (target 80-240).\")\n",
    "        return v\n",
    "\n",
    "class SearchHit(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str] = None\n",
    "    summary: str\n",
    "    score: float\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    answer: str = Field(..., min_length=10)\n",
    "    sources: List[str] = Field(default_factory=list, description=\"List of URLs used as sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bff575",
   "metadata": {},
   "source": [
    "## Tools\n",
    "We define three tools using the `@tool` decorator and log tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_tool(name: str):\n",
    "    TOOL_CALLS.append(name)\n",
    "\n",
    "def _clean_page_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).strip()\n",
    "    return text[:20000]\n",
    "\n",
    "@tool\n",
    "def fetch_web_page(url: str) -> str:\n",
    "    \"\"\"Fetch and clean the content of a web page at a given URL. Use when the user provides a URL you should index.\"\"\"\n",
    "    _log_tool(\"fetch_web_page\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (HomeworkAgent/1.0)\"}\n",
    "    r = requests.get(url, headers=headers, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return _clean_page_text(r.text)\n",
    "\n",
    "@tool\n",
    "def save_summary(url: str, summary: str) -> str:\n",
    "    \"\"\"Validate and save a concise summary (80-240 words) for a URL to the local summaries/ folder as JSON.\"\"\"\n",
    "    _log_tool(\"save_summary\")\n",
    "    # Try to capture title\n",
    "    title = None\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (HomeworkAgent/1.0)\"}\n",
    "        r = requests.get(url, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        t = soup.find(\"title\")\n",
    "        if t and t.text:\n",
    "            title = t.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    payload = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"summary\": summary.strip(),\n",
    "        \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "    record = SummaryRecord(**payload)  # Pydantic validation\n",
    "\n",
    "    out_path = SUM_DIR / f\"{slugify_url(url)}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record.model_dump(mode=\"json\"), f, ensure_ascii=False, indent=2)\n",
    "    return f\"Saved: {out_path.as_posix()}\"\n",
    "\n",
    "def _load_all_summaries() -> List[SummaryRecord]:\n",
    "    items: List[SummaryRecord] = []\n",
    "    for p in SUM_DIR.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            items.append(SummaryRecord(**data))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return items\n",
    "\n",
    "def _score(text: str, query: str) -> float:\n",
    "    q_tokens = [t for t in re.split(r\"\\W+\", query.lower()) if t]\n",
    "    t_tokens = [t for t in re.split(r\"\\W+\", text.lower()) if t]\n",
    "    if not q_tokens or not t_tokens:\n",
    "        return 0.0\n",
    "    hits = sum(t_tokens.count(qt) for qt in q_tokens)\n",
    "    return float(hits) / (len(t_tokens) + 1)\n",
    "\n",
    "@tool\n",
    "def search_summaries(query: str, k: int = 5) -> str:\n",
    "    \"\"\"Search previously saved summaries for the most relevant items and return top-k as JSON.\"\"\"\n",
    "    _log_tool(\"search_summaries\")\n",
    "    records = _load_all_summaries()\n",
    "    scored = []\n",
    "    for rec in records:\n",
    "        score = _score((rec.title or \"\") + \" \" + rec.summary, query)\n",
    "        if score > 0:\n",
    "            scored.append(SearchHit(url=rec.url, title=rec.title, summary=rec.summary, score=score))\n",
    "    scored.sort(key=lambda x: x.score, reverse=True)\n",
    "    top = scored[: max(1, min(k, 10))]\n",
    "    return json.dumps([h.model_dump(mode=\"json\") for h in top], ensure_ascii=False, indent=2)\n",
    "\n",
    "TOOLS = [fetch_web_page, save_summary, search_summaries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abbd79",
   "metadata": {},
   "source": [
    "## Agent Instructions\n",
    "Rules reflect local summary search and ordered behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cf039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a focused research assistant that uses tools to index web pages and answer questions from saved summaries.\n",
      "\n",
      "RULES:\n",
      "1) If the user asks to index/save pages (one or more URLs present), process them SEQUENTIALLY:\n",
      "   For each URL:\n",
      "   - Call fetch_web_page(url).\n",
      "   - Write an 80-240 word summary.\n",
      "   - Call save_summary(url, summary). If it fails validation, revise the summary and try ONCE more, then continue to the next URL.\n",
      "2) If the user asks a question (no new URLs to index), first call search_summaries(query, k=5) to retrieve the most relevant saved summaries, then answer ONLY from those summaries. Cite the specific URLs you used.\n",
      "3) Keep saved summaries 80-240 words. If your draft exceeds 240 words, shorten to ~200â€“240 words.\n",
      "4) Be concise, neutral, and avoid speculation.\n",
      "\n",
      "TOOLS:\n",
      "- fetch_web_page(url: str) -> str\n",
      "- save_summary(url: str, summary: str) -> str\n",
      "- search_summaries(query: str, k: int = 5) -> str\n",
      "\n",
      "OUTPUT:\n",
      "- Return a structured response with fields: answer (string) and sources (list of URLs).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AGENT_INSTRUCTIONS = \"\"\"\n",
    "You are a focused research assistant that uses tools to index web pages and answer questions from saved summaries.\n",
    "\n",
    "RULES:\n",
    "1) If the user asks to index/save pages (one or more URLs present), process them SEQUENTIALLY:\n",
    "   For each URL:\n",
    "   - Call fetch_web_page(url).\n",
    "   - Write an 80-240 word summary.\n",
    "   - Call save_summary(url, summary). If it fails validation, revise the summary and try ONCE more, then continue to the next URL.\n",
    "2) If the user asks a question (no new URLs to index), first call search_summaries(query, k=5) to retrieve the most relevant saved summaries, then answer ONLY from those summaries. Cite the specific URLs you used.\n",
    "3) Keep saved summaries 80-240 words. If your draft exceeds 240 words, shorten to ~200â€“240 words.\n",
    "4) Be concise, neutral, and avoid speculation.\n",
    "\n",
    "TOOLS:\n",
    "- fetch_web_page(url: str) -> str\n",
    "- save_summary(url: str, summary: str) -> str\n",
    "- search_summaries(query: str, k: int = 5) -> str\n",
    "\n",
    "OUTPUT:\n",
    "- Return a structured response with fields: answer (string) and sources (list of URLs).\n",
    "\"\"\"\n",
    "print(AGENT_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33549137",
   "metadata": {},
   "source": [
    "## Build the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67316d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=TOOLS,\n",
    "    system_prompt=AGENT_INSTRUCTIONS,\n",
    "    response_format=ProviderStrategy(QAResponse)\n",
    ")\n",
    "\n",
    "def extract_tool_calls(messages) -> List[str]:\n",
    "    names = []\n",
    "    for msg in messages or []:\n",
    "        tc = getattr(msg, \"tool_calls\", None)\n",
    "        if tc:\n",
    "            for call in tc:\n",
    "                name = call.get(\"name\") if isinstance(call, dict) else getattr(call, \"name\", None)\n",
    "                if name:\n",
    "                    names.append(name)\n",
    "    return names\n",
    "\n",
    "def run_agent_user_prompt(text: str):\n",
    "    TOOL_CALLS.clear()\n",
    "    try:\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "            config={\"recursion_limit\": 60}  # headroom\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Agent error:\", e)\n",
    "        if TOOL_CALLS:\n",
    "            print(\"Tools used before error:\", TOOL_CALLS)\n",
    "        raise\n",
    "    messages = result.get(\"messages\") if isinstance(result, dict) else None\n",
    "    final_text = \"\"\n",
    "    if messages:\n",
    "        last = messages[-1]\n",
    "        final_text = getattr(last, \"content\", str(last))\n",
    "    structured = result.get(\"structured_response\") if isinstance(result, dict) else None\n",
    "    tools_from_messages = extract_tool_calls(messages or [])\n",
    "    tools_used = list(dict.fromkeys(TOOL_CALLS + tools_from_messages))\n",
    "    return final_text, tools_used, structured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d823a",
   "metadata": {},
   "source": [
    "## Question 5 - Capybara Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54aab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q5: Tools Used ===\n",
      " - fetch_web_page\n",
      " - save_summary\n",
      "\n",
      "=== Q5: Structured Answer ===\n",
      "answer='The capybara (Hydrochoerus hydrochaeris) is the largest rodent species, native to South America. It inhabits savannas and dense forests near water bodies, living in social groups of 10-20 individuals, sometimes up to 100. Capybaras are herbivores, primarily grazing on grasses and aquatic plants, and are known for their excellent swimming abilities. They communicate through various vocalizations and have a unique social structure, with dominant males protecting females. While not currently threatened, they face hunting pressures in some regions. Capybaras have adapted well to urban environments and are often found in zoos. Their meat is consumed in some cultures, particularly during Lent in Venezuela.' sources=['https://en.wikipedia.org/wiki/Capybara']\n",
      "\n",
      "Saved summaries:\n",
      " - summaries/https_en.wikipedia.org_wiki_Capybara.json\n"
     ]
    }
   ],
   "source": [
    "q5_user_prompt = \"What is this page about? https://en.wikipedia.org/wiki/Capybara\"\n",
    "final_text, tools_used, structured = run_agent_user_prompt(q5_user_prompt)\n",
    "\n",
    "print(\"\\n=== Q5: Tools Used ===\")\n",
    "for t in tools_used:\n",
    "    print(\" -\", t)\n",
    "\n",
    "print(\"\\n=== Q5: Structured Answer ===\")\n",
    "print(structured if structured else final_text)\n",
    "\n",
    "print(\"\\nSaved summaries:\")\n",
    "for p in sorted(SUM_DIR.glob(\"*.json\")):\n",
    "    print(\" -\", p.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b504d",
   "metadata": {},
   "source": [
    "## Question 6 - Index Related Pages + Search Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd3105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q6: Tools Used ===\n",
      " - fetch_web_page\n",
      " - save_summary\n",
      " - search_summaries\n",
      "\n",
      "=== Q6: Structured Answer ===\n",
      "answer='Capybara populations face several threats, primarily from hunting and habitat loss. While the capybara (Hydrochoerus hydrochaeris) is not currently classified as threatened, it experiences hunting pressures in certain regions, particularly for its meat, which is consumed in various cultures, especially during Lent in Venezuela. Additionally, habitat destruction due to agricultural expansion and urban development poses significant risks to their natural environments. The Lesser Capybara (Hydrochoerus isthmius) is classified as Data Deficient by the IUCN, indicating insufficient data on its population status, which may also reflect underlying threats to its survival. Overall, while capybaras have adapted well to some urban environments, ongoing threats from hunting and habitat degradation remain critical concerns for their populations.' sources=['https://en.wikipedia.org/wiki/Hydrochoerus', 'https://en.wikipedia.org/wiki/Lesser_capybara', 'https://en.wikipedia.org/wiki/Capybara']\n",
      "\n",
      "Saved summaries now:\n",
      " - summaries/https_en.wikipedia.org_wiki_Capybara.json\n",
      " - summaries/https_en.wikipedia.org_wiki_Caviodon.json\n",
      " - summaries/https_en.wikipedia.org_wiki_Hydrochoerus.json\n",
      " - summaries/https_en.wikipedia.org_wiki_Lesser_capybara.json\n",
      " - summaries/https_en.wikipedia.org_wiki_Neochoerus.json\n",
      " - summaries/https_en.wikipedia.org_wiki_Neochoerus_aesopi.json\n"
     ]
    }
   ],
   "source": [
    "pages = [\n",
    "    \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
    "    \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus\",\n",
    "    \"https://en.wikipedia.org/wiki/Caviodon\",\n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus_aesopi\",\n",
    "]\n",
    "\n",
    "q6_user_prompt = (\n",
    "    \"Index the following pages by fetching each and then saving a concise summary for each:\\n\"\n",
    "    + \"\\n\".join(pages)\n",
    "    + \"\\n\\nThen answer this:\\n\"\n",
    "    \"What are threats to capybara populations? Provide a short synthesis and cite the specific pages you used.\"\n",
    ")\n",
    "\n",
    "final_text, tools_used, structured = run_agent_user_prompt(q6_user_prompt)\n",
    "\n",
    "print(\"\\n=== Q6: Tools Used ===\")\n",
    "for t in tools_used:\n",
    "    print(\" -\", t)\n",
    "\n",
    "print(\"\\n=== Q6: Structured Answer ===\")\n",
    "print(structured if structured else final_text)\n",
    "\n",
    "print(\"\\nSaved summaries now:\")\n",
    "for p in sorted(SUM_DIR.glob(\"*.json\")):\n",
    "    print(\" -\", p.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249fe0d",
   "metadata": {},
   "source": [
    "## Inspect Saved Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50166c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https_en.wikipedia.org_wiki_Capybara.json \n",
      " {\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Capybara\",\n",
      "  \"title\": \"Capybara - Wikipedia\",\n",
      "  \"summary\": \"The capybara (Hydrochoerus hydrochaeris) is the largest rodent species, native to South America. It inhabits savannas and dense forests near water bodies, living in social groups of 10-20 individuals, sometimes up to 100. Capybaras are herbivores, primarily grazing on grasses and aquatic plants, and are known for their excellent swimming abilities. They communicate through various vocalizations and have a unique social structure, with dominant males protecting females. While not currently threatened, they face hunting pressures in some regions. Capybaras have adapted well to urban environments and are often found in zoos. Their meat is consumed in some cultures, particularly during Lent in Venezuela.\",\n",
      "  \"timestamp\": \"2025-10-28T03:29:53.725205Z\"\n",
      "} \n",
      "\n",
      "https_en.wikipedia.org_wiki_Caviodon.json \n",
      " {\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Caviodon\",\n",
      "  \"title\": \"Caviodon - Wikipedia\",\n",
      "  \"summary\": \"Caviodon is an extinct genus of rodents that lived from the Late Miocene to the Late Pliocene. Related to modern capybaras, fossils of Caviodon have been discovered in Argentina, Venezuela, and Brazil. This genus contributes to the understanding of rodent evolution in South America, particularly in relation to the Hydrochoerinae subfamily. The study of Caviodon helps paleontologists understand the diversity and adaptations of prehistoric rodents in various environments, shedding light on their ecological roles during their time. Fossils indicate that these rodents thrived in diverse habitats.\",\n",
      "  \"timestamp\": \"2025-10-28T03:30:46.245665Z\"\n",
      "} \n",
      "\n",
      "https_en.wikipedia.org_wiki_Hydrochoerus.json \n",
      " {\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
      "  \"title\": \"Hydrochoerus - Wikipedia\",\n",
      "  \"summary\": \"Hydrochoerus is a genus of rodents that includes the capybara, the largest living rodent. Capybaras are semi-aquatic and inhabit regions near lakes, rivers, and swamps in South America. They are social animals, often living in groups of up to 100, and communicate through various vocalizations. Their diet primarily consists of grasses, and they have a gestation period of 130-150 days, typically giving birth to two to eight young. The genus includes two extant species: the common capybara (Hydrochoerus hydrochaeris) and the lesser capybara (Hydrochoerus isthmius).\",\n",
      "  \"timestamp\": \"2025-10-28T03:30:32.434673Z\"\n",
      "} \n",
      "\n",
      "https_en.wikipedia.org_wiki_Lesser_capybara.json \n",
      " {\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
      "  \"title\": \"Lesser capybara - Wikipedia\",\n",
      "  \"summary\": \"The Lesser Capybara (Hydrochoerus isthmius) is a semi-aquatic rodent found in South America, primarily in areas near water such as marshes and ponds. It is smaller than the common capybara, with adults reaching up to 3 feet in length and weighing around 62 pounds. This species is herbivorous, feeding mainly on grasses and aquatic plants, and has adapted to evade predators by being excellent swimmers. The Lesser Capybara is classified as Data Deficient by the IUCN, indicating a lack of sufficient data on its population status.\",\n",
      "  \"timestamp\": \"2025-10-28T03:30:18.926542Z\"\n",
      "} \n",
      "\n",
      "https_en.wikipedia.org_wiki_Neochoerus.json \n",
      " {\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Neochoerus\",\n",
      "  \"title\": \"Neochoerus - Wikipedia\",\n",
      "  \"summary\": \"Neochoerus is an extinct genus of rodents closely related to modern capybaras. Fossil remains have been found in North America and South America, indicating a wide distribution during the Pleistocene. The genus includes several species, such as Neochoerus aesopi, which lived in North America until about 12,000 years ago. These rodents are significant for understanding the evolutionary history of the Hydrochoerinae subfamily, providing insights into the adaptations and ecological roles of ancient capybara relatives. Their fossils help illustrate the diversity of prehistoric rodent life.\",\n",
      "  \"timestamp\": \"2025-10-28T03:30:42.163934Z\"\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preview(path, n=40):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read()\n",
    "    print(path.name, \"\\n\", txt[: min(len(txt), n*80)], \"\\n\")\n",
    "\n",
    "all_jsons = sorted(SUM_DIR.glob(\"*.json\"))\n",
    "for p in itertools.islice(all_jsons, 0, 5):\n",
    "    preview(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
